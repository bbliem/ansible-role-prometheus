# https://awesome-prometheus-alerts.grep.to/rules.html
groups:
- name: nodes
  rules:
  - alert: HostOutOfMemory
    expr: node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes * 100 < 10
    for: 2m
    labels:
      severity: warning
    annotations:
      summary: "Host out of memory (instance {{ "{{$labels.instance }}" }})"
      description: "Node memory is filling up (< 10% left)\n  VALUE = {{ "{{$value }}" }}\n  LABELS: {{ "{{$labels }}" }}"

  - alert: HostMemoryUnderMemoryPressure
    expr: rate(node_vmstat_pgmajfault[1m]) > 1000
    for: 2m
    labels:
      severity: warning
    annotations:
      summary: "Host memory under memory pressure (instance {{ "{{$labels.instance }}" }})"
      description: "The node is under heavy memory pressure. High rate of major page faults\n  VALUE = {{ "{{$value }}" }}\n  LABELS: {{ "{{$labels }}" }}"

  - alert: HostUnusualNetworkThroughputIn
    expr: sum by (instance) (irate(node_network_receive_bytes_total[2m])) / 1024 / 1024 > 100
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Host unusual network throughput in (instance {{ "{{$labels.instance }}" }})"
      description: "Host network interfaces are probably receiving too much data (> 100 MB/s)\n  VALUE = {{ "{{$value }}" }}\n  LABELS: {{ "{{$labels }}" }}"

  - alert: HostUnusualNetworkThroughputOut
    expr: sum by (instance) (irate(node_network_transmit_bytes_total[2m])) / 1024 / 1024 > 100
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Host unusual network throughput out (instance {{ "{{$labels.instance }}" }})"
      description: "Host network interfaces are probably sending too much data (> 100 MB/s)\n  VALUE = {{ "{{$value }}" }}\n  LABELS: {{ "{{$labels }}" }}"

  - alert: HostUnusualDiskReadRate
    expr: sum by (instance) (irate(node_disk_read_bytes_total[2m])) / 1024 / 1024 > 100
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Host unusual disk read rate (instance {{ "{{$labels.instance }}" }})"
      description: "Disk is probably reading too much data (> 100 MB/s)\n  VALUE = {{ "{{$value }}" }}\n  LABELS: {{ "{{$labels }}" }}"

  - alert: HostUnusualDiskWriteRate
    expr: sum by (instance) (irate(node_disk_written_bytes_total[2m])) / 1024 / 1024 > 50
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Host unusual disk write rate (instance {{ "{{$labels.instance }}" }})"
      description: "Disk is probably writing too much data (> 50 MB/s)\n  VALUE = {{ "{{$value }}" }}\n  LABELS: {{ "{{$labels }}" }}"

  - alert: HostOutOfDiskSpace
    expr: (node_filesystem_avail_bytes{mountpoint="/"}  * 100) / node_filesystem_size_bytes{mountpoint="/"} < 5
    for: 2m
    labels:
      severity: warning
    annotations:
      summary: "Host root out of disk space (instance {{ "{{$labels.instance }}" }})"
      description: "Disk at / is almost full (< 5% left)\n  VALUE = {{ "{{$value }}" }}\n  LABELS: {{ "{{$labels }}" }}"

  - alert: HostDiskWillFillIn4Hours
    expr: predict_linear(node_filesystem_free_bytes{fstype!~"tmpfs"}[1h], 4 * 3600) < 0
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Host disk will fill in 4 hours (instance {{ "{{$labels.instance }}" }})"
      description: "Disk will fill in 4 hours at current write rate\n  VALUE = {{ "{{$value }}" }}\n  LABELS: {{ "{{$labels }}" }}"

  - alert: HostDiskWillFillIn4Days
    expr: quantile_over_time(0.9, (-node_filesystem_avail_bytes{fstype!~'tmpfs', mountpoint!~'/boot/efi'}/deriv(node_filesystem_avail_bytes{fstype!~'tmpfs',mountpoint!~'/boot/efi'}[30m]))[4h:])/(60*60*24) < 4
    for: 15m
    labels:
      severity: warning
    annotations:
      summary: "Host disk will fill in 4 days (instance {{ "{{$labels.instance }}" }})"
      description: "Disk will fill in 4 days at current write rate\n  VALUE = {{ "{{$value }}" }}\n  LABELS: {{ "{{$labels }}" }}"

  - alert: HostDiskWillFillIn1Day
    expr: quantile_over_time(0.9, (-node_filesystem_avail_bytes{fstype!~'tmpfs', mountpoint!~'/boot/efi'}/deriv(node_filesystem_avail_bytes{fstype!~'tmpfs',mountpoint!~'/boot/efi'}[30m]))[4h:])/(60*60*24) < 1
    for: 15m
    labels:
      severity: error
    annotations:
      summary: "Host disk will fill in 1 day (instance {{ "{{$labels.instance }}" }})"
      description: "Disk will fill in 1 day at current write rate\n  VALUE = {{ "{{$value }}" }}\n  LABELS: {{ "{{$labels }}" }}"

  - alert: HostOutOfInodes
    expr: node_filesystem_files_free{mountpoint ="/"} / node_filesystem_files{mountpoint ="/"} * 100 < 10
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Host out of inodes (instance {{ "{{$labels.instance }}" }})"
      description: "Disk is almost running out of available inodes (< 10% left)\n  VALUE = {{ "{{$value }}" }}\n  LABELS: {{ "{{$labels }}" }}"

  - alert: HostUnusualDiskReadLatency
    expr: rate(node_disk_read_time_seconds_total[1m]) / rate(node_disk_reads_completed_total[1m]) > 100
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Host unusual disk read latency (instance {{ "{{$labels.instance }}" }})"
      description: "Disk latency is growing (read operations > 100ms)\n  VALUE = {{ "{{$value }}" }}\n  LABELS: {{ "{{$labels }}" }}"

  - alert: HostUnusualDiskWriteLatency
    expr: rate(node_disk_write_time_seconds_total[1m]) / rate(node_disk_writes_completed_total[1m]) > 100
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Host unusual disk write latency (instance {{ "{{$labels.instance }}" }})"
      description: "Disk latency is growing (write operations > 100ms)\n  VALUE = {{ "{{$value }}" }}\n  LABELS: {{ "{{$labels }}" }}"

  - alert: HostHighCpuLoad
    expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[10m])) * 100) > 80
    for: 10m
    labels:
      severity: warning
    annotations:
      summary: "Host high CPU load (instance {{ "{{$labels.instance }}" }})"
      description: "CPU load is > 80%\n  VALUE = {{ "{{$value }}" }}\n  LABELS: {{ "{{$labels }}" }}"

  - alert: HostSwapIsFillingUp
    expr: (1 - (node_memory_SwapFree_bytes / node_memory_SwapTotal_bytes)) * 100 > 80
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Host swap is filling up (instance {{ "{{$labels.instance }}" }})"
      description: "Swap is filling up (>80%)\n  VALUE = {{ "{{$value }}" }}\n  LABELS: {{ "{{$labels }}" }}"

  - alert: HostSystemdServiceCrashed
    expr: node_systemd_unit_state{state="failed"} > 0
    labels:
      severity: warning
    annotations:
      summary: "Host systemd service crashed (instance {{ "{{$labels.instance }}" }})"
      description: "systemd service crashed\n  VALUE = {{ "{{$value }}" }}\n  LABELS: {{ "{{$labels }}" }}"

  - alert: HostOomKillDetected
    expr: increase(node_vmstat_oom_kill[30m]) > 1
    labels:
      severity: warning
    annotations:
      summary: "Host OOM kill detected (instance {{ "{{$labels.instance }}" }})"
      description: "OOM kill detected\n  VALUE = {{ "{{$value }}" }}\n  LABELS: {{ "{{$labels }}" }}"


- name: postgresql
  rules:
  - alert: PostgresqlDown
    expr: pg_up == 0
    for: 5m
    labels:
      severity: error
    annotations:
      summary: "Postgresql down (instance {{ "{{$labels.instance }}" }})"
      description: "Postgresql instance is down\n  VALUE = {{ "{{$value }}" }}\n  LABELS: {{ "{{$labels }}" }}"

  - alert: PostgresqlRestarted
    expr: time() - pg_postmaster_start_time_seconds < 60
    for: 5m
    labels:
      severity: error
    annotations:
      summary: "Postgresql restarted (instance {{ "{{$labels.instance }}" }})"
      description: "Postgresql restarted\n  VALUE = {{ "{{$value }}" }}\n  LABELS: {{ "{{$labels }}" }}"

  - alert: PostgresqlExporterError
    expr: pg_exporter_last_scrape_error > 0
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Postgresql exporter error (instance {{ "{{$labels.instance }}" }})"
      description: "Postgresql exporter is showing errors. A query may be buggy in query.yaml\n  VALUE = {{ "{{$value }}" }}\n  LABELS: {{ "{{$labels }}" }}"

  - alert: PostgresqlTableNotVaccumed
    expr: time() - pg_stat_user_tables_last_autovacuum > 60 * 60 * 24
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Postgresql table not vaccumed (instance {{ "{{$labels.instance }}" }})"
      description: "Table has not been vaccum for 24 hours\n  VALUE = {{ "{{$value }}" }}\n  LABELS: {{ "{{$labels }}" }}"

  - alert: PostgresqlTableNotAnalyzed
    expr: time() - pg_stat_user_tables_last_autoanalyze > 60 * 60 * 24
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Postgresql table not analyzed (instance {{ "{{$labels.instance }}" }})"
      description: "Table has not been analyzed for 24 hours\n  VALUE = {{ "{{$value }}" }}\n  LABELS: {{ "{{$labels }}" }}"

  - alert: PostgresqlTooManyConnections
    expr: sum by (datname) (pg_stat_activity_count{datname!~"template.*|postgres"}) > pg_settings_max_connections * 0.9
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Postgresql too many connections (instance {{ "{{$labels.instance }}" }})"
      description: "PostgreSQL instance has too many connections\n  VALUE = {{ "{{$value }}" }}\n  LABELS: {{ "{{$labels }}" }}"

  - alert: PostgresqlDeadLocks
    expr: rate(pg_stat_database_deadlocks{datname!~"template.*|postgres"}[1m]) > 0
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Postgresql dead locks (instance {{ "{{$labels.instance }}" }})"
      description: "PostgreSQL has dead-locks\n  VALUE = {{ "{{$value }}" }}\n  LABELS: {{ "{{$labels }}" }}"

  - alert: PostgresqlSlowQueries
    expr: pg_slow_queries > 0
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Postgresql slow queries (instance {{ "{{$labels.instance }}" }})"
      description: "PostgreSQL executes slow queries\n  VALUE = {{ "{{$value }}" }}\n  LABELS: {{ "{{$labels }}" }}"

  - alert: PostgresqlHighRollbackRate
    expr: rate(pg_stat_database_xact_rollback{datname!~"template.*"}[3m]) / rate(pg_stat_database_xact_commit{datname!~"template.*"}[3m]) > 0.02
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Postgresql high rollback rate (instance {{ "{{$labels.instance }}" }})"
      description: "Ratio of transactions being aborted compared to committed is > 2 %\n  VALUE = {{ "{{$value }}" }}\n  LABELS: {{ "{{$labels }}" }}"

  - alert: PostgresqlLowXidConsumption
    expr: rate(pg_txid_current[1m]) < 5
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Postgresql low XID consumption (instance {{ "{{$labels.instance }}" }})"
      description: "Postgresql seems to be consuming transaction IDs very slowly\n  VALUE = {{ "{{$value }}" }}\n  LABELS: {{ "{{$labels }}" }}"

  - alert: PostgresqllowXlogConsumption
    expr: rate(pg_xlog_position_bytes[1m]) < 100
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Postgresqllow XLOG consumption (instance {{ "{{$labels.instance }}" }})"
      description: "Postgres seems to be consuming XLOG very slowly\n  VALUE = {{ "{{$value }}" }}\n  LABELS: {{ "{{$labels }}" }}"

  - alert: PostgresqlWaleReplicationStopped
    expr: rate(pg_xlog_position_bytes[1m]) == 0
    for: 5m
    labels:
      severity: error
    annotations:
      summary: "Postgresql WALE replication stopped (instance {{ "{{$labels.instance }}" }})"
      description: "WAL-E replication seems to be stopped\n  VALUE = {{ "{{$value }}" }}\n  LABELS: {{ "{{$labels }}" }}"

  - alert: PostgresqlHighRateStatementTimeout
    expr: rate(postgresql_errors_total{type="statement_timeout"}[5m]) > 3
    for: 5m
    labels:
      severity: error
    annotations:
      summary: "Postgresql high rate statement timeout (instance {{ "{{$labels.instance }}" }})"
      description: "Postgres transactions showing high rate of statement timeouts\n  VALUE = {{ "{{$value }}" }}\n  LABELS: {{ "{{$labels }}" }}"

  - alert: PostgresqlHighRateDeadlock
    expr: rate(postgresql_errors_total{type="deadlock_detected"}[1m]) * 60 > 1
    for: 5m
    labels:
      severity: error
    annotations:
      summary: "Postgresql high rate deadlock (instance {{ "{{$labels.instance }}" }})"
      description: "Postgres detected deadlocks\n  VALUE = {{ "{{$value }}" }}\n  LABELS: {{ "{{$labels }}" }}"

  - alert: PostgresqlTooManyDeadTuples
    expr: ((pg_stat_user_tables_n_dead_tup > 10000) / (pg_stat_user_tables_n_live_tup + pg_stat_user_tables_n_dead_tup)) >= 0.1 unless ON(instance) (pg_replication_is_replica == 1)
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Postgresql too many dead tuples (instance {{ "{{$labels.instance }}" }})"
      description: "PostgreSQL dead tuples is too large\n  VALUE = {{ "{{$value }}" }}\n  LABELS: {{ "{{$labels }}" }}"

  - alert: PostgresqlSplitBrain
    expr: count(pg_replication_is_replica == 0) != 1
    for: 5m
    labels:
      severity: error
    annotations:
      summary: "Postgresql split brain (instance {{ "{{$labels.instance }}" }})"
      description: "Split Brain, too many primary Postgresql databases in read-write mode\n  VALUE = {{ "{{$value }}" }}\n  LABELS: {{ "{{$labels }}" }}"

  - alert: PostgresqlPromotedNode
    expr: pg_replication_is_replica and changes(pg_replication_is_replica[1m]) > 0
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Postgresql promoted node (instance {{ "{{$labels.instance }}" }})"
      description: "Postgresql standby server has been promoted as primary node\n  VALUE = {{ "{{$value }}" }}\n  LABELS: {{ "{{$labels }}" }}"

  - alert: PostgresqlSslCompressionActive
    expr: sum(pg_stat_ssl_compression) > 0
    for: 5m
    labels:
      severity: error
    annotations:
      summary: "Postgresql SSL compression active (instance {{ "{{$labels.instance }}" }})"
      description: "Database connections with SSL compression enabled. This may add significant jitter in replication delay. Replicas should turn off SSL compression via `sslcompression=0` in `recovery.conf`.\n  VALUE = {{ "{{$value }}" }}\n  LABELS: {{ "{{$labels }}" }}"

  - alert: PostgresqlTooManyLocksAcquired
    expr: ((sum (pg_locks_count)) / (pg_settings_max_locks_per_transaction * pg_settings_max_connections)) > 0.20
    for: 5m
    labels:
      severity: error
    annotations:
      summary: "Postgresql too many locks acquired (instance {{ "{{$labels.instance }}" }})"
      description: "Too many locks acquired on the database. If this alert happens frequently, we may need to increase the postgres setting max_locks_per_transaction.\n  VALUE = {{ "{{$value }}" }}\n  LABELS: {{ "{{$labels }}" }}"


- name: prometheus
  rules:
  - alert: PrometheusTargetMissing
    expr: up == 0
    for: 1m
    labels:
      severity: error
    annotations:
      summary: "Prometheus target missing (instance {{ "{{ $labels.instance }}" }})"
      description: "A Prometheus target has disappeared. An exporter might be crashed.\n  VALUE = {{ "{{ $value }}" }}\n  LABELS: {{ "{{ $labels }}" }}"

  - alert: PrometheusAllTargetsMissing
    expr: count by (job) (up) == 0
    labels:
      severity: error
    annotations:
      summary: "Prometheus all targets missing (instance {{ "{{ $labels.instance }}" }})"
      description: "A Prometheus job does not have living target anymore.\n  VALUE = {{ "{{ $value }}" }}\n  LABELS: {{ "{{ $labels }}" }}"

  - alert: PrometheusConfigurationReloadFailure
    expr: prometheus_config_last_reload_successful != 1
    labels:
      severity: warning
    annotations:
      summary: "Prometheus configuration reload failure (instance {{ "{{ $labels.instance }}" }})"
      description: "Prometheus configuration reload error\n  VALUE = {{ "{{ $value }}" }}\n  LABELS: {{ "{{ $labels }}" }}"

  - alert: PrometheusTooManyRestarts
    expr: changes(process_start_time_seconds{job=~"prometheus|pushgateway|alertmanager"}[15m]) > 2
    labels:
      severity: warning
    annotations:
      summary: "Prometheus too many restarts (instance {{ "{{ $labels.instance }}" }})"
      description: "Prometheus has restarted more than twice in the last 15 minutes. It might be crashlooping.\n  VALUE = {{ "{{ $value }}" }}\n  LABELS: {{ "{{ $labels }}" }}"

  - alert: PrometheusAlertmanagerConfigurationReloadFailure
    expr: alertmanager_config_last_reload_successful != 1
    labels:
      severity: warning
    annotations:
      summary: "Prometheus AlertManager configuration reload failure (instance {{ "{{ $labels.instance }}" }})"
      description: "AlertManager configuration reload error\n  VALUE = {{ "{{ $value }}" }}\n  LABELS: {{ "{{ $labels }}" }}"

  - alert: PrometheusAlertmanagerConfigNotSynced
    expr: count(count_values("config_hash", alertmanager_config_hash)) > 1
    labels:
      severity: warning
    annotations:
      summary: "Prometheus AlertManager config not synced (instance {{ "{{ $labels.instance }}" }})"
      description: "Configurations of AlertManager cluster instances are out of sync\n  VALUE = {{ "{{ $value }}" }}\n  LABELS: {{ "{{ $labels }}" }}"

  - alert: PrometheusAlertmanagerE2eDeadManSwitch
    expr: vector(1)
    labels:
      severity: error
    annotations:
      summary: "Prometheus AlertManager E2E dead man switch (instance {{ "{{ $labels.instance }}" }})"
      description: "Prometheus DeadManSwitch is an always-firing alert. It's used as an end-to-end test of Prometheus through the Alertmanager.\n  VALUE = {{ "{{ $value }}" }}\n  LABELS: {{ "{{ $labels }}" }}"

  - alert: PrometheusNotConnectedToAlertmanager
    expr: prometheus_notifications_alertmanagers_discovered < 1
    labels:
      severity: error
    annotations:
      summary: "Prometheus not connected to alertmanager (instance {{ "{{ $labels.instance }}" }})"
      description: "Prometheus cannot connect the alertmanager\n  VALUE = {{ "{{ $value }}" }}\n  LABELS: {{ "{{ $labels }}" }}"

  - alert: PrometheusRuleEvaluationFailures
    expr: increase(prometheus_rule_evaluation_failures_total[3m]) > 0
    labels:
      severity: error
    annotations:
      summary: "Prometheus rule evaluation failures (instance {{ "{{ $labels.instance }}" }})"
      description: "Prometheus encountered {{ "{{ $value }}" }} rule evaluation failures, leading to potentially ignored alerts.\n  VALUE = {{ "{{ $value }}" }}\n  LABELS: {{ "{{ $labels }}" }}"

  - alert: PrometheusTemplateTextExpansionFailures
    expr: increase(prometheus_template_text_expansion_failures_total[3m]) > 0
    labels:
      severity: error
    annotations:
      summary: "Prometheus template text expansion failures (instance {{ "{{ $labels.instance }}" }})"
      description: "Prometheus encountered {{ "{{ $value }}" }} template text expansion failures\n  VALUE = {{ "{{ $value }}" }}\n  LABELS: {{ "{{ $labels }}" }}"

  - alert: PrometheusRuleEvaluationSlow
    expr: prometheus_rule_group_last_duration_seconds > prometheus_rule_group_interval_seconds
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Prometheus rule evaluation slow (instance {{ "{{ $labels.instance }}" }})"
      description: "Prometheus rule evaluation took more time than the scheduled interval. I indicates a slower storage backend access or too complex query.\n  VALUE = {{ "{{ $value }}" }}\n  LABELS: {{ "{{ $labels }}" }}"

  - alert: PrometheusNotificationsBacklog
    expr: min_over_time(prometheus_notifications_queue_length[10m]) > 0
    labels:
      severity: warning
    annotations:
      summary: "Prometheus notifications backlog (instance {{ "{{ $labels.instance }}" }})"
      description: "The Prometheus notification queue has not been empty for 10 minutes\n  VALUE = {{ "{{ $value }}" }}\n  LABELS: {{ "{{ $labels }}" }}"

  - alert: PrometheusAlertmanagerNotificationFailing
    expr: rate(alertmanager_notifications_failed_total[1m]) > 0
    labels:
      severity: error
    annotations:
      summary: "Prometheus AlertManager notification failing (instance {{ "{{ $labels.instance }}" }})"
      description: "Alertmanager is failing sending notifications\n  VALUE = {{ "{{ $value }}" }}\n  LABELS: {{ "{{ $labels }}" }}"

  - alert: PrometheusTargetEmpty
    expr: prometheus_sd_discovered_targets == 0
    labels:
      severity: error
    annotations:
      summary: "Prometheus target empty (instance {{ "{{ $labels.instance }}" }})"
      description: "Prometheus has no target in service discovery\n  VALUE = {{ "{{ $value }}" }}\n  LABELS: {{ "{{ $labels }}" }}"

  - alert: PrometheusTargetScrapingSlow
    expr: prometheus_target_interval_length_seconds{quantile="0.9"} > 60
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Prometheus target scraping slow (instance {{ "{{ $labels.instance }}" }})"
      description: "Prometheus is scraping exporters slowly\n  VALUE = {{ "{{ $value }}" }}\n  LABELS: {{ "{{ $labels }}" }}"

  - alert: PrometheusLargeScrape
    expr: increase(prometheus_target_scrapes_exceeded_sample_limit_total[10m]) > 10
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Prometheus large scrape (instance {{ "{{ $labels.instance }}" }})"
      description: "Prometheus has many scrapes that exceed the sample limit\n  VALUE = {{ "{{ $value }}" }}\n  LABELS: {{ "{{ $labels }}" }}"

  - alert: PrometheusTargetScrapeDuplicate
    expr: increase(prometheus_target_scrapes_sample_duplicate_timestamp_total[5m]) > 0
    labels:
      severity: warning
    annotations:
      summary: "Prometheus target scrape duplicate (instance {{ "{{ $labels.instance }}" }})"
      description: "Prometheus has many samples rejected due to duplicate timestamps but different values\n  VALUE = {{ "{{ $value }}" }}\n  LABELS: {{ "{{ $labels }}" }}"

  - alert: PrometheusTsdbCheckpointCreationFailures
    expr: increase(prometheus_tsdb_checkpoint_creations_failed_total[3m]) > 0
    labels:
      severity: error
    annotations:
      summary: "Prometheus TSDB checkpoint creation failures (instance {{ "{{ $labels.instance }}" }})"
      description: "Prometheus encountered {{ "{{ $value }}" }} checkpoint creation failures\n  VALUE = {{ "{{ $value }}" }}\n  LABELS: {{ "{{ $labels }}" }}"

  - alert: PrometheusTsdbCheckpointDeletionFailures
    expr: increase(prometheus_tsdb_checkpoint_deletions_failed_total[3m]) > 0
    labels:
      severity: error
    annotations:
      summary: "Prometheus TSDB checkpoint deletion failures (instance {{ "{{ $labels.instance }}" }})"
      description: "Prometheus encountered {{ "{{ $value }}" }} checkpoint deletion failures\n  VALUE = {{ "{{ $value }}" }}\n  LABELS: {{ "{{ $labels }}" }}"

  - alert: PrometheusTsdbCompactionsFailed
    expr: increase(prometheus_tsdb_compactions_failed_total[3m]) > 0
    labels:
      severity: error
    annotations:
      summary: "Prometheus TSDB compactions failed (instance {{ "{{ $labels.instance }}" }})"
      description: "Prometheus encountered {{ "{{ $value }}" }} TSDB compactions failures\n  VALUE = {{ "{{ $value }}" }}\n  LABELS: {{ "{{ $labels }}" }}"

  - alert: PrometheusTsdbHeadTruncationsFailed
    expr: increase(prometheus_tsdb_head_truncations_failed_total[3m]) > 0
    labels:
      severity: error
    annotations:
      summary: "Prometheus TSDB head truncations failed (instance {{ "{{ $labels.instance }}" }})"
      description: "Prometheus encountered {{ "{{ $value }}" }} TSDB head truncation failures\n  VALUE = {{ "{{ $value }}" }}\n  LABELS: {{ "{{ $labels }}" }}"

  - alert: PrometheusTsdbReloadFailures
    expr: increase(prometheus_tsdb_reloads_failures_total[3m]) > 0
    labels:
      severity: error
    annotations:
      summary: "Prometheus TSDB reload failures (instance {{ "{{ $labels.instance }}" }})"
      description: "Prometheus encountered {{ "{{ $value }}" }} TSDB reload failures\n  VALUE = {{ "{{ $value }}" }}\n  LABELS: {{ "{{ $labels }}" }}"

  - alert: PrometheusTsdbWalCorruptions
    expr: increase(prometheus_tsdb_wal_corruptions_total[3m]) > 0
    labels:
      severity: error
    annotations:
      summary: "Prometheus TSDB WAL corruptions (instance {{ "{{ $labels.instance }}" }})"
      description: "Prometheus encountered {{ "{{ $value }}" }} TSDB WAL corruptions\n  VALUE = {{ "{{ $value }}" }}\n  LABELS: {{ "{{ $labels }}" }}"

  - alert: PrometheusTsdbWalTruncationsFailed
    expr: increase(prometheus_tsdb_wal_truncations_failed_total[3m]) > 0
    labels:
      severity: error
    annotations:
      summary: "Prometheus TSDB WAL truncations failed (instance {{ "{{ $labels.instance }}" }})"
      description: "Prometheus encountered {{ "{{ $value }}" }} TSDB WAL truncation failures\n  VALUE = {{ "{{ $value }}" }}\n  LABELS: {{ "{{ $labels }}" }}"


- name: blackbox
  rules:
  - alert: BlackboxProbeFailed
    expr: probe_success == 0
    for: 4m
    labels:
      severity: error
    annotations:
      summary: Blackbox probe failed (instance {{ "{{ $labels.instance }}" }})
      description: "Probe failed\n  VALUE = {{ "{{ $value }}" }}\n  LABELS = {{ "{{ $labels }}" }}"

  - alert: BlackboxSlowProbe
    expr: avg_over_time(probe_duration_seconds[5m]) > 5
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Blackbox slow probe (instance {{ "{{ $labels.instance }}" }})"
      description: "Blackbox probe took more than 5s to complete\n  VALUE = {{ "{{ $value }}" }}\n  LABELS: {{ "{{ $labels }}" }}"

  - alert: BlackboxProbeHttpFailure
    expr: probe_http_status_code <= 199 OR probe_http_status_code >= 400
    for: 4m
    labels:
      severity: error
    annotations:
      summary: "Blackbox probe HTTP failure (instance {{ "{{ $labels.instance }}" }})"
      description: "HTTP status code is not 200-399\n  VALUE = {{ "{{ $value }}" }}\n  LABELS: {{ "{{ $labels }}" }}"

  - alert: BlackboxSslCertificateWillExpireSoon
    expr: probe_ssl_earliest_cert_expiry - time() < 86400 * 10
    labels:
      severity: warning
    annotations:
      summary: "Blackbox SSL certificate will expire soon (instance {{ "{{ $labels.instance }}" }})"
      description: "SSL certificate expires in 10 days\n  VALUE = {{ "{{ $value }}" }}\n  LABELS: {{ "{{ $labels }}" }}"

  - alert: BlackboxSslCertificateWillExpireSoon
    expr: probe_ssl_earliest_cert_expiry - time() < 86400 * 5
    labels:
      severity: error
    annotations:
      summary: "Blackbox SSL certificate will expire soon (instance {{ "{{ $labels.instance }}" }})"
      description: "SSL certificate expires in 5 days\n  VALUE = {{ "{{ $value }}" }}\n  LABELS: {{ "{{ $labels }}" }}"

  - alert: BlackboxSslCertificateExpired
    expr: probe_ssl_earliest_cert_expiry - time() <= 0
    labels:
      severity: error
    annotations:
      summary: "Blackbox SSL certificate expired (instance {{ "{{ $labels.instance }}" }})"
      description: "SSL certificate has expired already\n  VALUE = {{ "{{ $value }}" }}\n  LABELS: {{ "{{ $labels }}" }}"

  - alert: BlackboxProbeSlowHttp
    expr: avg_over_time(probe_http_duration_seconds[5m]) > 2
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Blackbox probe slow HTTP (instance {{ "{{ $labels.instance }}" }})"
      description: "HTTP request took more than 2s\n  VALUE = {{ "{{ $value }}" }}\n  LABELS: {{ "{{ $labels }}" }}"
